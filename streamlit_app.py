import streamlit as st
import requests
import time
import base64
import pandas as pd
import re

class DataForSEOAPI:
    def __init__(self, username, password):
        self.username = username
        self.password = password
        auth = base64.b64encode(f"{username}:{password}".encode()).decode()
        self.headers = {
            'Authorization': f'Basic {auth}',
            'Content-Type': 'application/json'
        }
    
    def post_task(self, query, location="Paris,Ile-de-France,France", language="fr"):
        endpoint = "https://api.dataforseo.com/v3/serp/google/organic/task_post"
        
        # Structure correcte de la requ√™te
        payload = [
            {
                "keyword": query,
                "location_name": location,
                "language_name": language,
                "device": "desktop",
                "os": "windows",
                "depth": 200  # Correspond au nombre de r√©sultats demand√©s
            }
        ]
        
        try:
            response = requests.post(
                endpoint,
                json=payload,  # Utilisation correcte de json=payload
                headers=self.headers
            )
            return response.json()
        except Exception as e:
            return {"error": str(e)}
    
    def get_results(self, task_id):
        endpoint = f"https://api.dataforseo.com/v3/serp/google/organic/task_get/{task_id}"
        try:
            response = requests.get(endpoint, headers=self.headers)
            return response.json()
        except Exception as e:
            return {"error": str(e)}


def scrape_google_urls(query, city, max_results=200, progress_bar=None):
    username = st.secrets["DATAFORSEO_USERNAME"]
    password = st.secrets["DATAFORSEO_PASSWORD"]
    api = DataForSEOAPI(username, password)
    
    # Cr√©ation de la t√¢che
    post_response = api.post_task(f"{query} {city}")
    
    # D√©bogage
    st.write("R√©ponse POST de l'API:", post_response)
    
    if "tasks" not in post_response or not post_response["tasks"]:
        st.error(f"Erreur dans la cr√©ation de la t√¢che: {post_response}")
        return None
    
    task_id = post_response["tasks"][0]["id"]
    
    # Param√®tres pour la r√©cup√©ration des r√©sultats
    max_attempts = 15  # Augment√© pour donner plus de temps √† l'API
    wait_time = 45  # Temps d'attente initial
    
    # Attente et r√©cup√©ration des r√©sultats
    for attempt in range(max_attempts):
        time.sleep(wait_time)
        
        # Apr√®s la premi√®re tentative, augmenter le temps d'attente
        if attempt > 0:
            wait_time = 30
        
        # Mise √† jour de la barre de progression si fournie
        if progress_bar:
            progress_bar.progress((attempt + 1) / max_attempts)
        
        # R√©cup√©rer les r√©sultats
        get_response = api.get_results(task_id)
        
        # D√©bogage
        st.write("Structure compl√®te de la r√©ponse:")
        st.json(get_response)
        
        # V√©rification de la structure de la r√©ponse
        if "tasks" in get_response and get_response["tasks"]:
            task = get_response["tasks"][0]
            if "result" in task and task["result"]:
                organic_results = []
                
                # Extraire les r√©sultats organiques
                for item in task["result"]:
                    if "items" in item and item["items"]:
                        for organic_item in item["items"]:
                            if organic_item["type"] == "organic":
                                organic_results.append({
                                    "position": organic_item.get("rank_absolute", ""),
                                    "title": organic_item.get("title", ""),
                                    "url": organic_item.get("url", ""),
                                    "description": organic_item.get("description", ""),
                                    "city": city,
                                    "query": query
                                })
                
                return organic_results[:max_results]  # Limiter au nombre de r√©sultats demand√©s
        else:
            st.write("Structure de r√©ponse inattendue")
    
    # Si on arrive ici, c'est qu'on n'a pas r√©ussi √† r√©cup√©rer les r√©sultats
    st.error("Impossible de r√©cup√©rer les r√©sultats apr√®s plusieurs tentatives")
    return None


def extract_domain(url):
    match = re.search(r'https?://(?:www\.)?([^/]+)', url)
    if match:
        return match.group(1)
    return url


def process_results(results):
    if not results:
        return None
    
    # Cr√©ation du DataFrame
    df = pd.DataFrame(results)
    
    # Ajout de la colonne domaine
    df['domain'] = df['url'].apply(extract_domain)
    
    return df


def main():
    st.title("üîç Scraper Google Search via DataForSEO (Standard Queue)")
    
    # Test API si coch√©
    if st.sidebar.checkbox("Tester connexion API"):
        username = st.secrets["DATAFORSEO_USERNAME"]
        password = st.secrets["DATAFORSEO_PASSWORD"]
        api = DataForSEOAPI(username, password)
        
        test_response = requests.get(
            "https://api.dataforseo.com/v3/merchant/account_info", 
            headers=api.headers
        )
        st.sidebar.write("Test de connexion API:", test_response.status_code)
        st.sidebar.json(test_response.json())
        return
    
    # Entr√©e utilisateur
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("Entrez votre terme de recherche")
        query = st.text_input("", value="avocat", help="Terme de recherche (exemple: avocat)")
    
    with col2:
        st.write("Liste des villes (une par ligne)")
        cities_text = st.text_area("", value="Paris", height=120, help="Entrez une ville par ligne")
    
    cities = [city.strip() for city in cities_text.split("\n") if city.strip()]
    
    # Nombre de r√©sultats par ville
    st.write("Nombre de r√©sultats √† r√©cup√©rer par ville")
    max_results = st.slider("", 10, 200, 200, help="Maximum de r√©sultats √† r√©cup√©rer par ville")
    
    # Bouton pour lancer le scraping
    if st.button("üîç Lancer les recherches", type="primary"):
        if not query or not cities:
            st.error("Veuillez entrer un terme de recherche et au moins une ville")
            return
        
        all_results = []
        progress_bar = st.progress(0)
        
        for i, city in enumerate(cities):
            st.write(f"Recherche en cours pour : {query} {city} (Peut prendre jusqu'√† 5 minutes)")
            results = scrape_google_urls(query, city, max_results, progress_bar)
            
            if results:
                all_results.extend(results)
                progress_bar.progress((i + 1) / len(cities))
            else:
                st.warning(f"Aucun r√©sultat trouv√© pour {query} {city}")
        
        if all_results:
            df = process_results(all_results)
            
            # Affichage des r√©sultats
            st.success(f"{len(all_results)} r√©sultats r√©cup√©r√©s au total")
            st.dataframe(df)
            
            # Export CSV
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "T√©l√©charger les r√©sultats (CSV)",
                csv,
                f"resultats_{query.replace(' ', '_')}.csv",
                "text/csv",
                key='download-csv'
            )
        else:
            st.error("Aucun r√©sultat trouv√©. Base toi sur cette docs pour essayer de corriger l'erreur : https://docs.dataforseo.com/v3/serp/google/organic/task_get/regular/?bash&_gl=1*12qovjf*_up*MQ..*_ga*MTcwODY0ODgwNC4xNzQzNDE5OTMx*_ga_T5NKP5Y695*MTc0MzQxOTkzMS4xLjEuMTc0MzQxOTk2OS4wLjAuMTA2MTE2MDU2MA..")


if __name__ == "__main__":
    main()
